# -*- coding: utf-8 -*-
"""ImageClassification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/Paimonz/e29d3da25dfb283e976eaa159a4a2d18/c-pia-de-imageclassification.ipynb
"""

import pathlib
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import tensorflow_hub as hub
print('TensorFlow version:', tf.__version__)
print('TensorFlow Hub version:', hub.__version__)

model_name = 'efficientnetv2-b2-21k'

models_path = {
    "efficientnetv2-b2-21k": "https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet21k_b2/feature_vector/2",
}

models_pixels = {
    "efficientnetv2-b2-21k": 260
}

model_path = models_path.get(model_name)
model_path

pixels = models_pixels.get(model_name)
pixels

image_size = (pixels, pixels)
image_size

print('Model: ', model_name)
print('Path: ', model_path)
print('Imagem size: ', image_size)

from keras.preprocessing.image import ImageDataGenerator

flow_from_directory = "/content/drive/MyDrive/Colab Notebooks/dataset"
data_directory = flow_from_directory
print(data_directory)

train_dataset = tf.keras.preprocessing.image_dataset_from_directory(data_directory,
                                                                    validation_split= .20,
                                                                    subset = 'training',
                                                                    label_mode='categorical',
                                                                    seed = 123,
                                                                    image_size=image_size,
                                                                    batch_size=1)

train_dataset.class_names

classes = train_dataset.class_names
classes

training_size = train_dataset.cardinality().numpy()
training_size

BATCH_SIZE = 16

train_dataset = train_dataset.unbatch().batch(BATCH_SIZE)
train_dataset

840/16

train_dataset = train_dataset.repeat()

from multiprocessing.process import parent_process
normalization_layer = tf.keras.layers.Rescaling(1. /255)
pre_processing = tf.keras.Sequential([normalization_layer])
pre_processing.add(tf.keras.layers.RandomRotation(40))
pre_processing.add(tf.keras.layers.RandomTranslation(0, 0.2))
pre_processing.add(tf.keras.layers.RandomTranslation(0.2, 0))
pre_processing.add(tf.keras.layers.RandomZoom(0.2, 0.2))
pre_processing.add(tf.keras.layers.RandomFlip(mode = 'horizontal'))

train_dataset = train_dataset.map(lambda images, labels: (pre_processing(images), labels))
train_dataset

test_dataset = tf.keras.preprocessing.image_dataset_from_directory(data_directory,
                                                                   validation_split= .20,
                                                                   subset ='validation',
                                                                   label_mode ='categorical',
                                                                   seed = 123,
                                                                   image_size = image_size,
                                                                   batch_size = 1)

test_size = test_dataset.cardinality().numpy()
test_size

test_dataset = test_dataset.unbatch().batch(BATCH_SIZE)
test_dataset = test_dataset.map(lambda images, labels: (pre_processing(images), labels))

model = tf.keras.Sequential([
                              tf.keras.layers.InputLayer(input_shape= image_size + (3,)),
                              hub.KerasLayer(model_path, trainable = False),
                              tf.keras.layers.Dropout(rate = 0.2),
                              tf.keras.layers.Dense(len(classes))
])

(None, ) + image_size + (3, )

model.build((None, )+ image_size + (3,))
model.summary()

model.compile(optimizer='adam', loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), metrics = 'accuracy')

steps_per_epoch = training_size // BATCH_SIZE
validation_steps = test_size // BATCH_SIZE
print(steps_per_epoch, validation_steps)

hist = model.fit(train_dataset, epochs= 10, steps_per_epoch = steps_per_epoch,
                 validation_data = test_dataset, validation_steps = validation_steps).history

plt.figure()
plt.ylabel('Loss (training and validation)')
plt.xlabel('Steps')
plt.plot(hist['loss'], label = 'training')
plt.plot(hist['val_loss'], label = 'testing')
plt.legend();

plt.figure()
plt.ylabel('Accuracy (training and validation)')
plt.xlabel('Steps')
plt.plot(hist['accuracy'], label = 'training')
plt.plot(hist['val_accuracy'], label = 'testing')
plt.legend();

x, y = next(iter(test_dataset))

x

x.shape

y

y.shape

image = x[0, :, :, :]
image

y_true = np.argmax(y[0])
y_true

classes[1]

plt.imshow(image)
plt.axis('off');

image.shape

image = np.expand_dims(image, axis = 0)
image.shape

prediction = model.predict(image)
prediction

prediction = np.argmax(prediction)
prediction



